%
% File naaclhlt2013.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{listings}
\usepackage{alltt}
\usepackage{balance}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newlength{\alglabelwidth}
\newcommand{\alginput}[1]{%
\par\noindent%
\settowidth{\alglabelwidth}{\emph{Output:}}%
\makebox[\alglabelwidth][l]{\emph{Input:}} \begin{tabular}[t]{l} #1 \end{tabular}}
\newcommand{\algoutput}[1]{%
\par\noindent%
\settowidth{\alglabelwidth}{\emph{Output:}}%
\makebox[\alglabelwidth][l]{\emph{Output:}} \begin{tabular}[t]{l} #1 \end{tabular}}
\newcommand{\algprecond}[1]{%
\par\noindent\textit{Initialization/Precondition: #1}}


\lstset{basicstyle=\ttfamily\footnotesize,       % the size of the fonts that are used for the code
		%numbers=left,                   % where to put the line-numbers
		numberblanklines=false
		numbersep=1em,                  % how far the line-numbers are from the code
		basewidth=0.52em,
		tabsize=4,  		% sets default tabsize to 2 spaces
		xleftmargin=\leftmargini
      }
\renewcommand*\thelstnumber{\the\value{lstnumber}:}
% END lstlisting environments

\lstnewenvironment{sql}[1][]{\lstset{language=SQL,gobble=4,emphstyle=\textit,#1}}{}


\setlength\titlebox{6.5cm}    % Expanding the titlebox

%\title{Large-scale Statistical Text Analytics in RDBMS\Thanks{This
%    submitting for double-blind reviewing.}}
\title{Towards In-database Statistical Text Analytics}

%\author{Kun Li, Christan Grant, Daisy Zhe Wang\\
	    %University of Florida\\
	    %111 Anywhere Street\\
	    %Gainesville, FL 32608, USA\\
	    %{\tt kli@cise.ufl.edu}
	  %\And
	%Sunny Khatri, George Chitouras\\
  	%Greenplum/EMC\\
  	%900 Main Street\\
	    %Gainesville, FL 32608, USA\\
  %{\tt george.chitouras@emc.com}}

\author{Barack Obama, Joe Biden\\
1700 Penslyvania Ave.\\
Washington D.C.\\
{\tt \{potus, vp\}@executivebranch.com}
\And
Mitt Romney, Paul Ryan\\
GOP Headquarters\\
310 First Street,\\
SE Washington, DC\\
{\tt \{mitt,paul\}@gop.com}
}


\date{}

\begin{document}
\maketitle
\begin{abstract}
Many companies keep large stores of text files and user logs in relational databases.
For these companies to perform analyics on these datasets, these companies must perform 
expensive transfers between databases and analyitic systems.
Additionally, may popular text analytics packages do not scale to production sized datasets.
In this paper, we introduce MADText, an open source in-database statistical text analytics module.
This module is a part of MADLib, a open source library for scalable in-database text analytics. 
%which is an open source project for statistical and parallel library for in-database analytics.  

Though this project, we motivate the use of text analytics inside the database.
We show that we can use the declarative SQL interface instead of procedural code to perform 
conditional random field based part-of-speech tagging and named-entity resolution. 
We design schema to store single and two-state features.
By using these novel schemas, we avoid costly feature re-compution for the same token over and over again.  
As far as we know, MADText is the first toolkit for statistical text analysis in relational database management systems.  
In the application level, we can support part-of-speech tagging(POS), named entity resolution and entity detection.  
We show that our package is linearly scalable and outperform the state of art packages by caching and avoiding costly re-compution.
%Lastly, we show that MADText can do nearly real time hot topics discovery for each statesof USA using streaming Tweets.
\end{abstract}

\section{Introduction}

%MapReduce systems e.g. Google's MapReduce, Apache Hadoop have gain more and more popularity since it was invented.
%All big companies are running hundreds of MapReduce jobs each day and each jobs contains terabytes of data.
%These data can be structured data such as transaction data and unstructured data such as natural text data. 
Traditional business intelligence has to pull data from databases into other massive 
data warehouses (OLAP) to analyze the data. 
The typical data movement process includes moving data from the dabases for use in other external 
data analysis tools, and then write the results back into database.
This movement process is expensive and minimization of data movement is a huge incentive for the datastore 
to be the same location as the analytic engine.
%This is especially true for large parallel systems \cite{ananthanarayanan2011disk}.
%is the place for data and also parallel database has the massive parallel processing engine. 
%So besides the data accounting functionality of 
%Relational database management systems (RDBMS) have long been excellent at data accounting.
%Recently, researchers have been pushing in-database analytic functionality to be integrated 
%into RDBMs to enable a deep insights into the data.

Newer parallel processing relational databases are can be leveraged more nodes and cores to handle large
increases in the size of data \cite{DeWitt:1992:PDS:129888.129894}. 
Motivated by these arguments, database researchers and database vendors are trying to make these practical,
investing in the ventures such as the MADlib project. 
MADlib is an open source library for scalable in-database analytics.
It provides parallel implementation of machine learning algorithms.

Text analytics has gained has more attention due to huge amount of text data generated from web, 
social networks every day.
Understanding this natural text data is crucial to business decision and even political campaign. 
Companies are analyzing text data to discover the popularity of specific topic and do sentiment analysis over certain products.
We enable statistical text analysis in parallel databases using linear chain conditional random fields, a
popular statistical method that enables
%probabilistic graphical model on real
natural language processing tasks such as part of speech tagging and named entity recognition on
real data \cite{DBLP:conf/icml/LaffertyMP01}.
We design a novel schema, that enables us to 
perform one time feature extraction and avoid re-computing the features for one token. 
We are able to use the falicity provided by a RDMBS to achieve the parallel CRF learning and inference for NLP tasks.

\section{Related Work}
Our work is inspired by a description of a unified architecture for in-RDBMS analytics \cite{Feng:2012:TUA:2213836.2213874}.
The paper points out that most of the machine learning algorithms can be expressed through
a unified architectures in RDBMs.
Essentially, the majority of machine learning algorithms can be reduced to a maximum likelihood objective function. % Multiple runs
The gradient vector and log likelihood can be calculated in parallel using user-defined aggregates supported in most of the RDBMS.
%User defined aggregates is MapReduce like framework to do parallel processing.
It also defines a driver function that manages the multi-pass optimization process until convergence criterion is met.

There are several implementations conditional random fields and but only a few 
large scale implementations for NLP tasks.
One example is the PCRFs \cite{phan2004flexcrfs} that are implemented over
massively parallel processing systems supporting Message Passing Interface (MPI) such as 
Cray XT3, SGI Altix, and IBM SP.
This is not implemented over RDBMs.

Other researchers have created systems for large scale text analytics including 
GATE, PurpleSox and SystemT
\cite{Cunningham2011a,Bohannon:2009:PSE:1519103.1519107,Li:2011:SDI:2002440.2002459}. 

%Wang's work on CRF inference Limited-memory BFGS We design novel relational schemas to store all the inputs, features, and outputs.

\section{Linear-chain CRF for IE in RDBMS}
Part-of-speech tagging (POS) is the process of assigning 
a part of speech to each word in a sentence. POS has been widely used in information retrieval, text to speech and dimension reduction. 
%There are two distinct methods for 
%POS task, rule-based and stochastic.
%In rule-based method, large collection of rules are defined to indentify the tag. Stochastic methods are based on 
%probabilistic graphic models such as hidden markov models and conditional random fields. 
In practice, conditional random fields are approved to achieve the state of art accuracy for the POS and NER tasks.

\subsection{System Architecture}
\begin{figure}
\centering
\includegraphics[height=15em]{system.png}
\caption{The MADText overall system architecture.}
\label{fig:systemarch}
\end{figure}

In Figure \ref{fig:systemarch} we show the system architecture for MADText~\footnote{Information on the MABLib process is available at http://madlib.net}. 
The top box contains the shows  the pipeline of the training phase. 
The second box shows the operational pipeline for the testing phase.
n the following sections we descripb each portion of the pipeline.

\subsection{In-database Implementation}
We use declarative SQL statements to generate all features for text.
%Any features in the state of art packages can be extracted using one single SQL clause.
%All of the common features described in literature can be extrated with one  SQL statement.
The extracted features are stored in a relation for either single or two-state features.
%M and R. M table stores the features involves two states. R table store all the single state features.
After the feature extraction, we use user-defined aggregates to calculate the gradient and log-likelihood in parallel using the transition function.
%We also implement the LBFGS in database. 


\subsection{Feature Extraction Using SQLs}
Text feature extraction is a step in most statistical text analysis methods.
We are able to implement all of the seven types of features used in POS and NER using exactly seven 
SQL statements. These features include: 
\begin{description}[noitemsep]
\item[Dictionary:] does this token exist in a dictionary? 
\item[Regex:] does this token match a regular expression? 
\item[Edge:] is the label of a token correlated with the label of a previous token? 
\item[Word:] does this token appear in the training data?
\item[Unknown:] does this token appeared in the training data below certain threshold? 
\item[Start/End:] is this token first/last in the token sequence?
%\item[End feature:] is this token the last in the token sequence?
\end{description}

There are many advantages for extracting features using SQLs.  
%Compared with procedure language, SQL is much more easier to understand. 
The SQL statements hide a lot of the complexity in present  in the actual operation.
It turns out that each type of feature can be extracted out using exactly one SQL statement, 
making the feature extraction code extremely succinct.  
From a pedagogical point of view, it is much easier to teach text feature extraction using our SQL feature extraction code.
Secondly, SQLs statements are naïvely parallel due to their set semantics.
For example, we compute features for each distinct token and avoid re-computing the features for repeated tokens.  
In Figure~\ref{fig:edgefeatures} and Figure~\ref{fig:regexfeatures} we show how to extract edge
and regex features, respectively.

\begin{figure}
\centering
\begin{lstlisting}[language=SQL,breaklines=true]
SELECT doc2.start_pos, doc2.doc_id, 'E.', 
       ARRAY[doc1.label, doc2.label]
FROM   segmenttbl doc1, segmenttbl doc2
WHERE  doc1.doc_id = doc2.doc_id AND 
       doc1.start_pos+1 = doc2.start_pos
\end{lstlisting}
\caption{Query for extracting edge features}
\label{fig:edgefeatures}
\end{figure}

\begin{figure}
\centering
\begin{lstlisting}[language=SQL, breaklines=true]
SELECT start_pos, doc_id, 'R_' || name, 
       ARRAY[-1, label]
FROM  regextbl, segmenttbl
WHERE seg_text ~ pattern
\end{lstlisting}
\caption{Query for extracting regex features}
\label{fig:regexfeatures}
\end{figure}


\begin{figure}
\centering
\begin{lstlisting}[language=SQL,breaklines=true]
INSERT INTO rtbl(start_pos,doc_id,feature)
SELECT start_pos, doc_id, 
       array_cat(fset.feature, 
   ARRAY[f_index,start_pos, 
   CASE 
       WHEN tmp1_feature.feature=fset.feature 
       THEN 1
   ELSE 0 END] )
FROM   tmp1_feature, featureset fset
WHERE  tmp1_feature.f_name = fset.f_name AND 
       fset.f_name <> 'E.';
\end{lstlisting}
\caption{Query for extracting regex features}
\end{figure}


\subsection{Parallel Linear-chain CRF Training}
\begin{algorithm} 
\caption{CRF training$(z_{1:M})$} \label{alg:CRF training}
\alginput{Observation set $z_{1:M}$,\\
convergence criterion $\mathit{Convergence}()$,\\
start strategy $\mathit{Start}()$,\\
initialization strategy $\mathit{Initialization}()$,\\
transition strategy $\mathit{Transition}()$,\\
finalization strategy $\mathit{Finalization}()$}
\algoutput{Coefficients $w \in R^N$}
\algprecond{$iteration = 0, diag = 1$}
\begin{algorithmic}[1]
\State $w_{new} \gets \mathit{Start}(z_{1:M})$
\Repeat
        \State $w_{old} \gets w_{new}$
        \State $\mathit{state} \gets \mathit{Initialization}(w_{new})$
\For{$m \in 1..M$} \Comment{Single entry in the observation set}
\State $\mathit{state} \gets \mathit{Transition}(\mathit{state}, z_m)$
                \Comment{Computing gradient and log-likelihood.}
\EndFor
\State $w_{new} \gets Finalization(\mathit{state})$ \Comment{Mainly invoke L-BFGS convex solver}
\Until{$Convergence(w_{new}, g_{new}, \mathit{iteration})$}
    \State \Return $w_{new}$
\end{algorithmic}
\label{algo:crftraining}
\end{algorithm}

\paragraph{Programming Model.}
In Algorithm~\ref{algo:crftraining} we show the parallel CRF training strategy, in the fashion of the selected programming model supported by MADlib (mainly user-defined aggregate).
%We provide above the algorithm of parallel CRF training strategy, in the fashion of the selected programming model supported by MADlib (mainly user-defined aggregate).

\paragraph{Parallelism.}
The outer loop is inherently sequential over multiple iterations.
The iteration $n+1$ takes the output of iteration $n$ as input, so on so forth until the stop criterion is satisfied.
The inner loop which calculates the gradient and log-likelihood for each document is data-parallel.
Simple model averaging are used to merge two states.
%A merge function is not explicitly added to the pseudocode for simplicity.
The finalization function invokes the L-BFGS convex solver to get a new solution. 
%L-BFGS is sequential, but very fast.
%Experiments show that the speed-up ration approaches the number of segments configured in the Greenplum database.

%\paragraph{Convergence criterion.}
%The following convergence criterions are supported.
%\begin{enumerate}
%    \item The norm of gradient divided by the norm of coefficient drops below a given threshold.
%    \item The maximum number of iterations is reached.
%\end{enumerate}

%\paragraph{Start strategy.}
%In most cases, zeros are used unless otherwise specified.

\paragraph{Transition strategies.}
This function contains the logic of computing the gradient and log-likelihood for each tuple using the forward-backward
algorithm. The algorithms will be discussed in the following sections.

\begin{algorithm}
\caption{transition-lbfgs$(\mathit{state}, z_m)$} \label{alg:transition-lbfgs}
\alginput{Transition state $\mathit{state}$,\\
observation entry $z_m$,\\
gradient function $\mathit{Gradient}()$}
\algoutput{Transition state $\mathit{state}$}
\begin{algorithmic}[1]
    \State $\{state.g,state.loglikelihood\} \gets \mathit{Gradient}(\mathit{state}, z_m)$
        \Comment{using forward-backward algorithm to calculate gradient and loglikelihood}
    \State $\mathit{state}.num\_rows \gets \mathit{state}.num\_rows + 1$
    \State \Return $\mathit{state}$
\end{algorithmic}
\label{algo:crftransition}
\end{algorithm}


\paragraph{Merge strategies.}
The merge function simply sums the gradient and log-likelihood over all training documents
%\begin{algorithm}
%\caption{merge-lbfgs$(\mathit{state_1}, \mathit{state_2})$} \label{alg:merge-lbfgs}
%\alginput{Transition state $\mathit{state_1}$,\\
%Transition state $\mathit{state_2}$}
%\algoutput{Transition state $\mathit{state_{new}}$}
%\begin{algorithmic}[1]
%    \State $\mathit{state_{new}}.g \gets \mathit{state_1}.g + \mathit{state_2}.g$
%    \State $\mathit{state_{new}}.loglikelihood \gets \mathit{state_1}.loglikelihood + \mathit{state_2}.loglikelihood$
%    \State \Return $\mathit{state_{new}}$
%\end{algorithmic}
%\end{algorithm}


\paragraph{Finalization strategy.}
The finalization function invokes the L-BFGS convex solver to get a new coefficent vector.\\
\begin{algorithm}
\caption{finalization-lbfgs$(state)$} \label{alg:CRF training}
\alginput{Transition state $state$,\\
LBFGS $\mathit{lbfgs}()$}
\algoutput{Transition state $state$}
\begin{algorithmic}[1]
        \State $\{state.g,state.loglikelihood\} \gets penalty(state.g,state.loglikelihood)$ \Comment{To avoid overfitting, add penalization}
        \State $\{state.g,state.loglikelihood\}\gets-\{state.g,state.loglikelihood\}$ \Comment{negation for maximization}
        \State LBFGS instance($state)$ \Comment{initialize the L-BFGS instance with previous state}
        \State instance.$lbfgs()$ \Comment{invoke the L-BFGS convex solver}
        \State instance.save\_state$(state)$ \Comment{save updated variables to the state for next iteration}
        \State \Return $state$
\end{algorithmic}
\label{algo:crffinal}
\end{algorithm}
%Feeding with current solution, gradient, log-likelihood, etc., the L-BFGS will ouput a new solution.
%To avoid overfitting, a penalization function is needed. We choose to penalize the log-likelihood with a spherical Gaussian weight prior.
%Also, L-BFGS is to maximum objective, so we need to negate the gradient vector and log-likelihood to fit our needs in order minimize the log-likehood.
%\paragraph{LBFGS Convex Optimization}
%The limited-memory BFGS(L-BFGS) is the limited memory variation of the Broyden-Fletcher-Goldfarb-Shanno(BFGS) algorithm which
%is the state of art of large scale non constraint convex optimization method.

Limited-memory BFGS (L-BFGS), a variation of the Broyden-Fletcher-Goldfarb-Shanno (BFGS)
algorithm is a leading method for large scale non-constraint convex optimization method.
We translate the in-memory Java implementation to C++ in-database implementation using Eigen support.
Eigen vector and Eigen matrix are used instead of the one dimensional and two-dimensional arrays.
In the Java in-memory implementation, it defines many static variables defined and shared between iterations.
However, in the MADlib implementation, we define these variables in the state object.
Before each iteration of L-BFGS optimization, we need to initialize the L-BFGS with the current state object. 
At the end of each iteration, we need to dump the updated variables to the database state for next iteration.

%\begin{lstlisting}[language=SQL,gobble=4]
%    select crf_train_data('/path/to/trainingdata')
%\end{lstlisting}

%\begin{lstlisting}[language=SQL,gobble=4]
%    select crf_train_fgen('train_data', 
%    'regex','dic','featuretbl','feature_dic')
%\end{lstlisting}

%\begin{lstlisting}[language=SQL,gobble=4]
%    select lincrf('featuretbl','sparse_r',
%    'dense_m','sparse_m','f_size',45, 
%    'feature_dic','feature',20)
%\end{lstlisting}

\subsection{Parallel Linear-chain CRF Inference}
 The Viterbi algorithm is to find the top-k most likely labellings of a document 
for CRF models. 
We chose to implement a SQL clause to drive the Viterbi inference. 
The Viterbi inference is implemented sequentially and each function call 
will finish labeling of one document.
However, in Greenplum, Viterbi can be run in parallel over different subsets 
of the document on a multi-core machine. So, the CRF inference is naively parallel. 
%\begin{lstlisting}[language=SQL,gobble=4]
%    SELECT doc_id, vcrf_top1(m.score,r.score)
%    FROM   mfactors as m ,rfactors as r
%\end{lstlisting}

%\paragraph{Inference}
%\begin{lstlisting}[language=SQL,gobble=4]
%    select crf_test_data('/path/to/testingdata')
%\end{lstlisting}

%\begin{lstlisting}[language=SQL,gobble=4]
%    select crf_test_fgen('test_data','dic',
%    'label','regex','feature','mtbl','rtbl')
%\end{lstlisting}

%\begin{lstlisting}[language=SQL,gobble=4]
%    select vcrf_label('test_data', 
%    'mtbl','rtbl','label','extraction')
%\end{lstlisting}

%\cite{DBLP:conf/icml/LaffertyMP01}
%\cite{DBLP:journals/scholarpedia/Viterbi09}
%\cite{DBLP:journals/siamjo/MoralesN00}
%\cite{DBLP:journals/coling/DeRose88}
%\cite{DBLP:conf/naacl/ShaP03}
%\cite{DBLP:journals/coling/MarcusSM94}


%\begin{thebibliography}{}
%$\[
%V(i,y) =
%\begin{cases}
%\max_{y^\prime}(V(i-1,y^\prime)) + \textstyle \sum_{k=1}^K \lambda_kf_k(y,y^\prime,x_i), & \text{if } i\ge0 \\
%0, & \text{if } i=-1.
%\end{cases}
%\]$

%\subsection{Entity Detection}

\section{Experiments and Results}
In order to evaluate the performance and scalability of the linear-chain CRF learning and inference on Greenplum,
we conduct experiments on various data sizes over on a 32-core machine with 2T hard drive and 64GB memory.
We use CoNLL2000 dataset containing 9000 tagged sentences for learning. 
This dataset is labeled with 45 POS tags.
%as the tag set. We split the dataset into 6 parts containing 1k, 2k, 4k, 6k, 8k, 9k sentences.
To evaluate the inference performance, we extracted 1 million sentences from the New York Times dataset. 
%news and truncated the inference datasets into 5 dataset contains 100k, 200k, 400k, 600k, 800k, 1000k sentences.

\begin{figure}
\centering
\includegraphics[height=11em]{training}
\caption{Linear-chain CRF training scalability}
\label{fig:crftrain}
\end{figure}

\begin{figure}
\centering
\includegraphics[height=11em]{training}
\caption{Linear-chain CRF inference scalability}
\label{fig:crftrain}
\end{figure}

%\section{User Case: Tweet Analysis}
%\label{sec:blind}

\section{Conclusion}
We show an approach for using the database as not just the location for
data storage but also as the location for statistical natural language 
computation.
We demonstrate this with the implementation of CRF inference and training 
for part of speech tagging.
We show that these algorithms scale extremly well with the number of cores
in a system.
The RDBMs and SQL interface enable functionalities desired by many 
applications such as real-time training and real-time inference, over
structured and unstructured data.




\section*{Acknowledgments}
Christan Grant is funded by a National
Science Foundation Graduate Research Fellowship under Grant No. DGE-0802270.
This work was supported by a gift from EMC Greenplum.

\bibliographystyle{naaclhlt2013}
%\bibliographystyle{plain}
\bibliography{citation}

\end{document}
